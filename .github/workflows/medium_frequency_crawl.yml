name: 数据采集-中频用户
on:
  schedule:
    # 每2小时的第10分钟运行，避开睡眠窗口 (UTC 17:00-22:59)
    - cron: '10 0,2,4,6,8,10,12,14,16,23 * * *'
  workflow_dispatch:
    inputs:
      max_workers:
        description: '最大并发线程数'
        required: false
        default: '2'
        type: choice
        options:
          - '1'
          - '2'
          - '3'

jobs:
  crawl_medium_frequency:
    runs-on: ubuntu-latest

    steps:
    - name: 检出代码
      uses: actions/checkout@v4

    - name: 设置Python环境
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    - name: 安装依赖
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: 执行中频爬取任务
      env:
        DB_HOST: ${{ secrets.DB_HOST }}
        DB_PORT: ${{ secrets.DB_PORT }}
        DB_NAME: ${{ secrets.DB_NAME }}
        DB_USER: ${{ secrets.DB_USER }}
        DB_PASSWORD: ${{ secrets.DB_PASSWORD || secrets.MYSQL_PASSWORD }}
        MYSQL_PASSWORD: ${{ secrets.MYSQL_PASSWORD || secrets.DB_PASSWORD }}
        RSS_HUB_URL: ${{ secrets.RSS_HUB_URL }}
        RSS_HUB_TOKEN: ${{ secrets.RSS_HUB_TOKEN }}
        CRAWL_HIGH_LIMIT: ${{ secrets.CRAWL_HIGH_LIMIT }}
        CRAWL_MEDIUM_LIMIT: ${{ secrets.CRAWL_MEDIUM_LIMIT }}
        CRAWL_LOW_LIMIT: ${{ secrets.CRAWL_LOW_LIMIT }}
        CRAWL_MEDIUM_INTERVAL_MINUTES: ${{ secrets.CRAWL_MEDIUM_INTERVAL_MINUTES }}
        CRAWL_LOW_INTERVAL_HOURS: ${{ secrets.CRAWL_LOW_INTERVAL_HOURS }}
        CRAWLER_REQUEST_DELAY_MIN: ${{ secrets.CRAWLER_REQUEST_DELAY_MIN }}
        CRAWLER_REQUEST_DELAY_MAX: ${{ secrets.CRAWLER_REQUEST_DELAY_MAX }}
      run: |
        python3.12 main.py --task medium_freq --max-workers ${{ github.event.inputs.max_workers || '2' }}
